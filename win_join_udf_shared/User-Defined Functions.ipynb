{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c6b445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Defined Functions\n",
    "# One of the most powerful things that you can do in Spark is define your own functions. These\n",
    "# user-defined functions (UDFs) make it possible for you to write your own custom\n",
    "# transformations using Python or Scala and even use external libraries. UDFs can take and return\n",
    "# one or more columns as input. Spark UDFs are incredibly powerful because you can write them\n",
    "# in several different programming languages; you do not need to create them in an esoteric format\n",
    "# or domain-specific language. They’re just functions that operate on the data, record by record.\n",
    "# By default, these functions are registered as temporary functions to be used in that specific\n",
    "# SparkSession or Context.\n",
    "\n",
    "# Although you can write UDFs in Scala, Python, or Java, there are performance considerations\n",
    "# that you should be aware of. To illustrate this, we’re going to walk through exactly what happens\n",
    "# when you create UDF, pass that into Spark, and then execute code using that UDF.\n",
    "# The first step is the actual function. We’ll create a simple one for this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c628922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing session from sql from pyspark to start the sessio\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# creating the seasion\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d315f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let’s write a power3 function that takes a number and raises it to a power of three:\n",
    "\n",
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "power3(2.0)\n",
    "\n",
    "# In this trivial example, we can see that our functions work as expected. We are able to provide an\n",
    "# individual input and produce the expected result (with this simple test case). Thus far, our\n",
    "# expectations for the input are high: it must be a specific type and cannot be a null value (see\n",
    "# “Working with Nulls in Data”).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f9dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Now that we’ve created these functions and tested them, we need to register them with Spark so\n",
    "# that we can use them on all of our worker machines. Spark will serialize the function on the\n",
    "# driver and transfer it over the network to all executor processes. This happens regardless of language.\n",
    "\n",
    "# When you use the function, there are essentially two different things that occur. If the function is\n",
    "# written in Scala or Java, you can use it within the Java Virtual Machine (JVM). This means that\n",
    "# there will be little performance penalty aside from the fact that you can’t take advantage of code\n",
    "# generation capabilities that Spark has for built-in functions. There can be performance issues if\n",
    "# you create or use a lot of objects; we cover that in the section on optimization in Chapter 19.\n",
    "# If the function is written in Python, something quite different happens. Spark starts a Python\n",
    "# process on the worker, serializes all of the data to a format that Python can understand\n",
    "# (remember, it was in the JVM earlier), executes the function row by row on that data in the\n",
    "# Python process, and then finally returns the results of the row operations to the JVM and Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be84634",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# WARNING\n",
    "# Starting this Python process is expensive, but the real cost is in serializing the data to Python. This is\n",
    "# costly for two reasons: it is an expensive computation, but also, after the data enters Python, Spark\n",
    "# cannot manage the memory of the worker. This means that you could potentially cause a worker to fail\n",
    "# if it becomes resource constrained (because both the JVM and Python are competing for memory on\n",
    "# the same machine). We recommend that you write your UDFs in Scala or Java—the small amount of\n",
    "# time it should take you to write the function in Scala will always yield significant speed ups, and on\n",
    "# top of that, you can still use the function from Python!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92afa20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now that you have an understanding of the process, let’s work through an example. First, we\n",
    "# need to register the function to make it available as a DataFrame function:\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88092763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Then, we can use it in our DataFrame code:\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "udfExampleDF.select(power3udf(col(\"num\"))).show(2)\n",
    "\n",
    "# At this juncture, we can use this only as a DataFrame function. That is to say, we can’t use it\n",
    "# within a string expression, only on an expression. However, we can also register this UDF as a\n",
    "# Spark SQL function. This is valuable because it makes it simple to use this function within SQL\n",
    "# as well as across languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83d52049",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Undefined function: power3. This function is neither a built-in/temporary function, nor a persistent function that is qualified as spark_catalog.default.power3.; line 1 pos 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35168/435515347.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# a DataFrame function, we use it as a SQL expression:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mudfExampleDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"power3(7)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselectExpr\u001b[0;34m(self, *expr)\u001b[0m\n\u001b[1;32m   2046\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m             \u001b[0mexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Undefined function: power3. This function is neither a built-in/temporary function, nor a persistent function that is qualified as spark_catalog.default.power3.; line 1 pos 0"
     ]
    }
   ],
   "source": [
    "\n",
    "# Because this function is registered with Spark SQL—and we’ve learned that any Spark SQL\n",
    "# function or expression is valid to use as an expression when working with DataFrames—we can\n",
    "# turn around and use the UDF that we wrote in Scala, in Python. However, rather than using it as\n",
    "# a DataFrame function, we use it as a SQL expression:\n",
    "    \n",
    "udfExampleDF.selectExpr(\"power3(7)\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2f51b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.power3(double_value)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# One thing we can also do to ensure that our functions are working correctly is specify a return\n",
    "# type. As we saw in the beginning of this section, Spark manages its own type information, which\n",
    "# does not align exactly with Python’s types. Therefore, it’s a best practice to define the return type\n",
    "# for your function when you define it. It is important to note that specifying the return type is not\n",
    "# necessary, but it is a best practice.If you specify the type that doesn’t align with the actual type returned by the function, Spark will\n",
    "# not throw an error but will just return null to designate a failure. \n",
    "# You can see this if you were to switch the return type in the following function to be a DoubleType:\n",
    "\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "spark.udf.register(\"power3py\", power3, DoubleType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca55828e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|power3py(num)|\n",
      "+-------------+\n",
      "|         null|\n",
      "|         null|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "udfExampleDF.selectExpr(\"power3py(num)\").show(2)\n",
    "# # registered via Python\n",
    "# This is because the range creates integers. When integers are operated on in Python, Python\n",
    "# won’t convert them into floats (the corresponding type to Spark’s double type), therefore we see\n",
    "# null. We can remedy this by ensuring that our Python function returns a float instead of an\n",
    "# integer and the function will behave correctly.\n",
    "# Naturally, we can use either of these from SQL, too, after we register them:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b197f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# -- in SQL\n",
    "# SELECT power3(12), power3py(12) -- doesn't work because of return type\n",
    "# When you want to optionally return a value from a UDF, you should return None in Python and\n",
    "# an Option type in Scala:\n",
    "\n",
    "# -- in SQL\n",
    "# CREATE TEMPORARY FUNCTION myFunc AS 'com.organization.hive.udf.FunctionName'\n",
    "# Additionally, you can register this as a permanent function in the Hive Metastore by removing\n",
    "# TEMPORARY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc854ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion\n",
    "# This chapter demonstrated how easy it is to extend Spark SQL to your own purposes and do so in\n",
    "# a way that is not some esoteric, domain-specific language but rather simple functions that are\n",
    "# easy to test and maintain without even using Spark! This is an amazingly powerful tool that you\n",
    "# can use to specify sophisticated business logic that can run on five rows on your local machinesor on terabytes of data on a 100-node cluster!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
