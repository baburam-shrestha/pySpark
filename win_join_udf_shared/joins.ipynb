{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aed7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Expressions\n",
    "# A join brings together two sets of data, the left and the right, by comparing the value of one or\n",
    "# more keys of the left and right and evaluating the result of a join expression that determines\n",
    "# whether Spark should bring together the left set of data with the right set of data. The most\n",
    "# common join expression, an equi-join, compares whether the specified keys in your left and\n",
    "# right datasets are equal. If they are equal, Spark will combine the left and right datasets. The\n",
    "# opposite is true for keys that do not match; Spark discards the rows that do not have matching\n",
    "# keys. Spark also allows for much more sophsticated join policies in addition to equi-joins. We\n",
    "# can even use complex types and perform something like checking whether a key exists within an\n",
    "# array when you perform a join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b39c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Types\n",
    "# Whereas the join expression determines whether two rows should join, the join type determines\n",
    "# what should be in the result set. There are a variety of different join types available in Spark for you to use:\n",
    "#     Inner joins (keep rows with keys that exist in the left and right datasets)\n",
    "#     Outer joins (keep rows with keys in either the left or right datasets)\n",
    "#     Left outer joins (keep rows with keys in the left dataset)\n",
    "#     Right outer joins (keep rows with keys in the right dataset)\n",
    "#     Left semi joins (keep the rows in the left, and only the left, dataset where the key appears in the right dataset)\n",
    "#     Left anti joins (keep the rows in the left, and only the left, dataset where they do not appear in the right dataset)\u0000\n",
    "#     Natural joins (perform a join by implicitly matching the columns between the two datasets with the same names)\n",
    "#     Cross (or Cartesian) joins (match every row in the left dataset with every row in the right dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89beeaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing session from sql from pyspark to start the sessio\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fe45468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/18 16:41:11 WARN Utils: Your hostname, HP-G62 resolves to a loopback address: 127.0.1.1; using 192.168.18.113 instead (on interface enp3s0)\n",
      "22/10/18 16:41:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/18 16:41:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/18 16:41:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/18 16:41:14 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/10/18 16:41:14 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/10/18 16:41:14 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "# creating the seasion\n",
    "spark = SparkSession.builder.appName(\"Joins\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "770822b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "person = spark.createDataFrame([\n",
    "    (0, \"Bill Chambers\", 0, [100]),\n",
    "    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "    (2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    "    .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
    "    .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "\n",
    "sparkStatus = spark.createDataFrame([\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\")])\\\n",
    "    .toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14ce7ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+\n",
      "| id|            name|graduate_program|   spark_status|\n",
      "+---+----------------+----------------+---------------+\n",
      "|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b8c96ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb72311c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|        status|\n",
      "+---+--------------+\n",
      "|500|Vice President|\n",
      "|250|    PMC Member|\n",
      "|100|   Contributor|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkStatus.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19c10571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s register these as tables so that we use them throughout the chapter:\n",
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c5004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Joins\n",
    "# in SQL\n",
    "# SELECT * FROM person JOIN graduateProgram\n",
    "# ON person.graduate_program = graduateProgram.id\n",
    "\n",
    "# Inner joins evaluate the keys in both of the DataFrames or tables and include (and join together)\n",
    "# only the rows that evaluate to true. In the following example, we join the graduateProgram\n",
    "#     person.join(graduateProgram, joinExpression, joinType).show()\n",
    "    \n",
    "# DataFrame with the person DataFrame to create a new DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90d94ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys that do not exist in both DataFrames will not show in the resulting DataFrame. For\n",
    "# example, the following expression would result in zero values in the resulting DataFrame:\n",
    "\n",
    "wrongJoinExpression = person[\"name\"] == graduateProgram[\"school\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fab5348",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28565/1399672180.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrongJoinExpression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "wrongJoinExpression.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "604c3d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Inner joins are the default join, so we just need to specify our left DataFrame and join the right in the JOIN expression:\n",
    "person.join(graduateProgram, person[\"graduate_program\"] == graduateProgram['id'],\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "280298eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:>                                                         (0 + 4) / 4]\r",
      "\r",
      "[Stage 14:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Inner joins are the default join, so we just need to specify our left DataFrame and join the right in the JOIN expression:\n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
    "joinType=\"inner\"\n",
    "person.join(graduateProgram, joinExpression,joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb0b6e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Outer Joins\n",
    "# in SQL\n",
    "# SELECT * FROM person FULL OUTER JOIN graduateProgram\n",
    "# ON graduate_program = graduateProgram.id\n",
    "\n",
    "# Outer joins evaluate the keys in both of the DataFrames or tables and includes (and joins together) the rows that\n",
    "# evaluate to true or false. If there is no equivalent row in either the left or right DataFrame, Spark will insert null:\n",
    "\n",
    "#     person.join(graduateProgram, joinExpression, joinType).show()\n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
    "joinType=\"outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1b107f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school|  id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|   0|   Bill Chambers|               0|          [100]|\n",
      "|  2|Masters|                EECS|UC Berkeley|null|            null|            null|           null|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   2|Michael Armbrust|               1|     [250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Left Outer Joins\n",
    "# in SQL\n",
    "# SELECT * FROM graduateProgram LEFT OUTER JOIN person\n",
    "# ON person.graduate_program = graduateProgram.id\n",
    "\n",
    "# Left outer joins evaluate the keys in both of the DataFrames or tables and includes all rows from\n",
    "# the left DataFrame as well as any rows in the right DataFrame that have a match in the left\n",
    "# DataFrame. If there is no equivalent row in the right DataFrame, Spark will insert null:\n",
    "    \n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
    "joinType = \"left_outer\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ea35117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 30:==============>                                           (1 + 3) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Right Outer Joins\n",
    "# in SQL\n",
    "# SELECT * FROM person RIGHT OUTER JOIN graduateProgram\n",
    "# ON person.graduate_program = graduateProgram.id\n",
    "\n",
    "# Right outer joins evaluate the keys in both of the DataFrames or tables and includes all rows\n",
    "# from the right DataFrame as well as any rows in the left DataFrame that have a match in the right\n",
    "# DataFrame. If there is no equivalent row in the left DataFrame, Spark will insert null:\n",
    "\n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
    "joinType = \"right_outer\"\n",
    "person.join(graduateProgram, joinExpression, joinType).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2239f520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 36:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Left Semi Joins\n",
    "# in SQL\n",
    "# SELECT * FROM gradProgram2 LEFT SEMI JOIN person\n",
    "# ON gradProgram2.id = person.graduate_program\n",
    "\n",
    "# Semi joins are a bit of a departure from the other joins. They do not actually include any values\n",
    "# from the right DataFrame. They only compare values to see if the value exists in the second\n",
    "# DataFrame. If the value does exist, those rows will be kept in the result, even if there are\n",
    "# duplicate keys in the left DataFrame. Think of left semi joins as filters on a DataFrame, as\n",
    "# opposed to the function of a conventional join:\n",
    "\n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
    "joinType = \"left_semi\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd7db450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------------+\n",
      "| id| degree|          department|           school|\n",
      "+---+-------+--------------------+-----------------+\n",
      "|  0|Masters|School of Informa...|      UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|      UC Berkeley|\n",
      "|  0|Masters|      Duplicated Row|Duplicated School|\n",
      "+---+-------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "gradProgram2 = graduateProgram.union(spark.createDataFrame([\n",
    "(0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")]))\n",
    "gradProgram2.createOrReplaceTempView(\"gradProgram2\")\n",
    "gradProgram2.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78abce64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n",
      "| id| degree|department|     school|\n",
      "+---+-------+----------+-----------+\n",
      "|  2|Masters|      EECS|UC Berkeley|\n",
      "+---+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Left Anti Joins\n",
    "# in SQL\n",
    "# SELECT * FROM graduateProgram LEFT ANTI JOIN person\n",
    "# ON graduateProgram.id = person.graduate_program\n",
    "\n",
    "# Left anti joins are the opposite of left semi joins. Like left semi joins, they do not actually\n",
    "# include any values from the right DataFrame. They only compare values to see if the value exists\n",
    "# in the second DataFrame. However, rather than keeping the values that exist in the second\n",
    "# DataFrame, they keep only the values that do not have a corresponding key in the second\n",
    "# DataFrame. Think of anti joins as a NOT IN SQL-style filter:\n",
    "\n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
    "joinType = \"left_anti\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac242b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Joins\n",
    "# in SQL\n",
    "# SELECT * FROM graduateProgram NATURAL JOIN person\n",
    "\n",
    "# Natural joins make implicit guesses at the columns on which you would like to join. It finds\n",
    "# matching columns and returns the results. Left, right, and outer natural joins are all supported.\n",
    "\n",
    "# WARNING\n",
    "# Implicit is always dangerous! The following query will give us incorrect results because the two\n",
    "# DataFrames/tables share a column name (id), but it means different things in the datasets. You should\n",
    "# always use this join with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2d4f23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 57:===========================================>              (3 + 1) / 4]\r",
      "\r",
      "[Stage 58:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school| id|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Cross (Cartesian) Joins\n",
    "# in SQL\n",
    "# SELECT * FROM graduateProgram CROSS JOIN person\n",
    "# ON graduateProgram.id = person.graduate_program\n",
    "\n",
    "# The last of our joins are cross-joins or cartesian products. Cross-joins in simplest terms are inner\n",
    "# joins that do not specify a predicate. Cross joins will join every single row in the left DataFrame\n",
    "# to ever single row in the right DataFrame. This will cause an absolute explosion in the number of\n",
    "# rows contained in the resulting DataFrame. If you have 1,000 rows in each DataFrame, the crossjoin of these will result in 1,000,000 (1,000 x 1,000) rows. For this reason, you must very\n",
    "# explicitly state that you want a cross-join by using the cross join keyword:\n",
    "    \n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
    "joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "492c2a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:=========================================>               (8 + 3) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  0|   Bill Chambers|               0|          [100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  0|   Bill Chambers|               0|          [100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# If you truly intend to have a cross-join, you can call that out explicitly:\n",
    "\n",
    "# in SQL\n",
    "# SELECT * FROM graduateProgram CROSS JOIN person\n",
    "\n",
    "person.crossJoin(graduateProgram).show()\n",
    "\n",
    "# WARNING\n",
    "# You should use cross-joins only if you are absolutely, 100 percent sure that this is the join you need.\n",
    "# There is a reason why you need to be explicit when defining a cross-join in Spark. They’re dangerous!\n",
    "# Advanced users can set the session-level configuration spark.sql.crossJoin.enable to true in\n",
    "# order to allow cross-joins without warnings or without Spark trying to perform another join for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222069c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f01f4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928ff27d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
