{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce3f6dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing session from sql from pyspark to start the sessio\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf8eb918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/18 16:54:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# creating the seasion\n",
    "spark = SparkSession.builder.appName(\"Joins Challenges\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2223bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When performing joins, there are some specific challenges and some common questions that\n",
    "# arise. The rest of the chapter will provide answers to these common questions and then explain\n",
    "# how, at a high level, Spark performs joins. This will hint at some of the optimizations that we are\n",
    "# going to cover in later parts of this book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "265e1b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "person = spark.createDataFrame([\n",
    "    (0, \"Bill Chambers\", 0, [100]),\n",
    "    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "    (2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    "    .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n",
    "    .toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "\n",
    "sparkStatus = spark.createDataFrame([\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\")])\\\n",
    "    .toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "955789b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43448aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60b02b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "879814a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s register these as tables so that we use them throughout the chapter:\n",
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f0e8a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===============================================>          (9 + 2) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|personId|            name|graduate_program|   spark_status| id|        status|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|       0|   Bill Chambers|               0|          [100]|100|   Contributor|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|500|Vice President|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|100|   Contributor|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Joins on Complex Types\n",
    "# in SQL\n",
    "# SELECT * FROM\n",
    "# (select id as personId, name, graduate_program, spark_status FROM person)\n",
    "# INNER JOIN sparkStatus ON array_contains(spark_status, id)\n",
    "\n",
    "# Even though this might seem like a challenge, it’s actually not. Any expression is a valid join\n",
    "# expression, assuming that it returns a Boolean:\n",
    "    \n",
    "from pyspark.sql.functions import expr\n",
    "person.withColumnRenamed(\"id\", \"personId\")\\\n",
    ".join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa185ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pyspark.sql.functions.col(col: str) -> pyspark.sql.column.Column>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b2d8633",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30329/2436209668.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mgradProgramDuplicate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraduateProgram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"graduate_program#40\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mjoinExpression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"graduate_program#1079\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mgradProgramDuplicate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"graduate_program#40\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Note that there are now two graduate_program columns, even though we joined on that key:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1986\u001b[0m         \"\"\"\n\u001b[1;32m   1987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1988\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1989\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m             )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'col'"
     ]
    }
   ],
   "source": [
    "# Handling Duplicate Column Names\n",
    "# One of the tricky things that come up in joins is dealing with duplicate column names in your\n",
    "# results DataFrame. In a DataFrame, each column has a unique ID within Spark’s SQL Engine,\n",
    "# Catalyst. This unique ID is purely internal and not something that you can directly reference.\n",
    "# This makes it quite difficult to refer to a specific column when you have a DataFrame with duplicate column names.\n",
    "# This can occur in two distinct situations:\n",
    "#     The join expression that you specify does not remove one key from one of the input DataFrames and the\n",
    "#         keys have the same column name\n",
    "#     Two columns on which you are not performing the join have the same name\n",
    "    \n",
    "# Let’s create a problem dataset that we can use to illustrate these problems:\n",
    "\n",
    "gradProgramDuplicate = graduateProgram.withColumnRenamed(\"id\", \"graduate_program#40\")\n",
    "\n",
    "joinExpression = person.col(\"graduate_program#1079\")==gradProgramDuplicate.col(\"graduate_program#40\")\n",
    "\n",
    "# Note that there are now two graduate_program columns, even though we joined on that key:\n",
    "person.join(gradProgramDuplicate, joinExpression).show()\n",
    "\n",
    "# The challenge arises when we refer to one of these columns:\n",
    "person.join(gradProgramDupe, joinExpression).select(\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "106148a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|graduate_program|\n",
      "+----------------+\n",
      "|               0|\n",
      "|               1|\n",
      "|               1|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Approach 1: Different join expression\n",
    "# When you have two keys that have the same name, probably the easiest fix is to change the join\n",
    "# expression from a Boolean expression to a string or sequence. This automatically removes one of\n",
    "# the columns for you during the join:\n",
    "    \n",
    "person.join(gradProgramDuplicate,\"graduate_program\").select(\"graduate_program\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da26a909",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30329/260147034.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# key names or if the source DataFrames have columns that simply have the same name:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mjoinExpression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"graduate_program\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgraduateProgram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mperson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradProgramDuplicate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoinExpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"graduate_program\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"graduate_program\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1986\u001b[0m         \"\"\"\n\u001b[1;32m   1987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1988\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1989\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m             )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'col'"
     ]
    }
   ],
   "source": [
    "# Approach 2: Dropping the column after the join\n",
    "# Another approach is to drop the offending column after the join. When doing this, we need to\n",
    "# refer to the column via the original source DataFrame. We can do this if the join uses the same\n",
    "# key names or if the source DataFrames have columns that simply have the same name:\n",
    "from pyspark.sql.functions import col\n",
    "joinExpression = person.col(\"graduate_program\") == graduateProgram.col(\"id\")  \n",
    "person.join(gradProgramDuplicate, joinExpression).drop(person.col(\"graduate_program\")).select(\"graduate_program\").show()\n",
    "\n",
    "person.join(graduateProgram, joinExpression).drop(graduateProgram.col(\"id\")).show()\n",
    "\n",
    "# This is an artifact of Spark’s SQL analysis process in which an explicitly referenced column will\n",
    "# pass analysis because Spark has no need to resolve the column. Notice how the column uses the .col method \n",
    "# instead of a column function. That allows us to implicitly specify that column by its specific ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af16a581",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30329/1805887678.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgradProgram3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraduateProgram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"grad_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mjoinExpression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"graduate_program\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgradProgram3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mperson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradProgram3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoinExpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1986\u001b[0m         \"\"\"\n\u001b[1;32m   1987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1988\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1989\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m             )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'col'"
     ]
    }
   ],
   "source": [
    "# Approach 3: Renaming a column before the join\n",
    "# We can avoid this issue altogether if we rename one of our columns before the join:\n",
    "    \n",
    "gradProgram3 = graduateProgram.withColumnRenamed(\"id\", \"grad_id\")\n",
    "joinExpression = person.col(\"graduate_program\") == gradProgram3.col(\"grad_id\")\n",
    "person.join(gradProgram3, joinExpression).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14050ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How Spark Performs Joins\n",
    "# To understand how Spark performs joins, you need to understand the two core resources at play:\n",
    "# the node-to-node communication strategy and per node computation strategy. These internals are\n",
    "# likely irrelevant to your business problem. However, comprehending how Spark performs joins\n",
    "# can mean the difference between a job that completes quickly and one that never completes at\n",
    "# all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communication Strategies\n",
    "# Spark approaches cluster communication in two different ways during joins. It either incurs a\n",
    "# shuffle join, which results in an all-to-all communication or a broadcast join. Keep in mind that\n",
    "# there is a lot more detail than we’re letting on at this point, and that’s intentional. Some of these\n",
    "# internal optimizations are likely to change over time with new improvements to the cost-based\n",
    "# optimizer and improved communication strategies. For this reason, we’re going to focus on the\n",
    "# high-level examples to help you understand exactly what’s going on in some of the more\n",
    "# common scenarios, and let you take advantage of some of the low-hanging fruit that you can use\n",
    "# right away to try to speed up some of your workloads.\n",
    "# The core foundation of our simplified view of joins is that in Spark you will have either a big\n",
    "# table or a small table. Although this is obviously a spectrum (and things do happen differently if\n",
    "# you have a “medium-sized table”), it can help to be binary about the distinction for the sake of\n",
    "# this explanation.\n",
    "\n",
    "#     Big table–to–big table\n",
    "#     When you join a big table to another big table, you end up with a shuffle join\n",
    "#     In a shuffle join, every node talks to every other node and they share data according to which\n",
    "#     node has a certain key or set of keys (on which you are joining). These joins are expensive\n",
    "#     because the network can become congested with traffic, especially if your data is not partitioned\n",
    "#     well.\n",
    "#     This join describes taking a big table of data and joining it to another big table of data. An\n",
    "#     example of this might be a company that receives billions of messages every day from the\n",
    "#     Internet of Things, and needs to identify the day-over-day changes that have occurred. The way\n",
    "#     to do this is by joining on deviceId, messageType, and date in one column, and date - 1 day\n",
    "#     in the other column.\n",
    "#     In Figure 8-1, DataFrame 1 and DataFrame 2 are both large DataFrames. This means that all\n",
    "#     worker nodes (and potentially every partition) will need to communicate with one another during\n",
    "#     the entire join process (with no intelligent partitioning of data).\n",
    "    \n",
    "#     Big table–to–small table\n",
    "#     When the table is small enough to fit into the memory of a single worker node, with some\n",
    "#     breathing room of course, we can optimize our join. Although we can use a big table–to–big\n",
    "#     table communication strategy, it can often be more efficient to use a broadcast join. What this\n",
    "#     means is that we will replicate our small DataFrame onto every worker node in the cluster (be it\n",
    "#     located on one machine or many). Now this sounds expensive. However, what this does is\n",
    "#     prevent us from performing the all-to-all communication during the entire join process. Instead,\n",
    "#     we perform it only once at the beginning and then let each individual worker node perform the\n",
    "#     work without having to wait or communicate with any other worker node, as is depicted in\n",
    "    \n",
    "#     At the beginning of this join will be a large communication, just like in the previous type of join.\n",
    "#     However, immediately after that first, there will be no further communication between nodes.\n",
    "#     This means that joins will be performed on every single node individually, making CPU the\n",
    "#     biggest bottleneck. For our current set of data, we can see that Spark has automatically set this up\n",
    "#     as a broadcast join by looking at the explain plan:\n",
    "#     val joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")\n",
    "#     person.join(graduateProgram, joinExpr).explain()\n",
    "#     == Physical Plan ==\n",
    "#     *BroadcastHashJoin [graduate_program#40], [id#5....\n",
    "#     :- LocalTableScan [id#38, name#39, graduate_progr...\n",
    "#     +- BroadcastExchange HashedRelationBroadcastMode(....\n",
    "#     +- LocalTableScan [id#56, degree#57, departmen....\n",
    "#     With the DataFrame API, we can also explicitly give the optimizer a hint that we would like to\n",
    "#     use a broadcast join by using the correct function around the small DataFrame in question. In this\n",
    "#     example, these result in the same plan we just saw; however, this is not always the case:\n",
    "#     import org.apache.spark.sql.functions.broadcast\n",
    "#     val joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")\n",
    "#     person.join(broadcast(graduateProgram), joinExpr).explain()\n",
    "#     The SQL interface also includes the ability to provide hints to perform joins. These are not\n",
    "#     enforced, however, so the optimizer might choose to ignore them. You can set one of these hints\n",
    "#     by using a special comment syntax. MAPJOIN, BROADCAST, and BROADCASTJOIN all do the same\n",
    "#     thing and are all supported:\n",
    "#     -- in SQL\n",
    "#     SELECT /*+ MAPJOIN(graduateProgram) */ * FROM person JOIN graduateProgram\n",
    "#     ON person.graduate_program = graduateProgram.id\n",
    "#     This doesn’t come for free either: if you try to broadcast something too large, you can crash your\n",
    "#     driver node (because that collect is expensive). This is likely an area for optimization in the\n",
    "#     future.\n",
    "                       \n",
    "#     Little table–to–little table\n",
    "#     When performing joins with small tables, it’s usually best to let Spark decide how to join them.\n",
    "#     You can always force a broadcast join if you’re noticing strange behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
