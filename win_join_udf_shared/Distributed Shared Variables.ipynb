{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8894c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed Shared Variables\n",
    "# In addition to the Resilient Distributed Dataset (RDD) interface, the second kind of low-level\n",
    "# API in Spark is two types of “distributed shared variables”: broadcast variables and\n",
    "# accumulators. These are variables you can use in your user-defined functions (e.g., in a map\n",
    "# function on an RDD or a DataFrame) that have special properties when running on a cluster.\n",
    "# Specifically, accumulators let you add together data from all the tasks into a shared result (e.g.,\n",
    "# to implement a counter so you can see how many of your job’s input records failed to parse),\n",
    "# while broadcast variables let you save a large value on all the worker nodes and reuse it across\n",
    "# many Spark actions without re-sending it to the cluster. This chapter discusses some of the\n",
    "# motivation for each of these variable types as well as how to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97a8296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast Variables\n",
    "# Broadcast variables are a way you can share an immutable value efficiently around the cluster\n",
    "# without encapsulating that variable in a function closure. The normal way to use a variable in\n",
    "# your driver node inside your tasks is to simply reference it in your function closures (e.g., in a\n",
    "# map operation), but this can be inefficient, especially for large variables such as a lookup table or\n",
    "# a machine learning model. The reason for this is that when you use a variable in a closure, it\n",
    "# must be deserialized on the worker nodes many times (one per task). Moreover, if you use the\n",
    "# same variable in multiple Spark actions and jobs, it will be re-sent to the workers with every job instead of once.\n",
    "# This is where broadcast variables come in. Broadcast variables are shared, immutable variables\n",
    "# that are cached on every machine in the cluster instead of serialized with every single task. The\n",
    "# canonical use case is to pass around a large lookup table that fits in memory on the executors and  use that in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6697f719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/19 09:52:03 WARN Utils: Your hostname, HP-G62 resolves to a loopback address: 127.0.1.1; using 192.168.18.113 instead (on interface enp3s0)\n",
      "22/10/19 09:52:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/19 09:52:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/19 09:52:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/19 09:52:06 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/10/19 09:52:06 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/10/19 09:52:06 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/10/19 09:52:06 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/10/19 09:52:06 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n"
     ]
    }
   ],
   "source": [
    "# importing session from sql from pyspark to start the sessio\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# creating the seasion\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cee42dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example, suppose that you have a list of words or values:\n",
    "\n",
    "my_collection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
    "words = spark.sparkContext.parallelize(my_collection, 2)\n",
    "type(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28ead963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Spark': 1000, 'Definitive': 200, 'Big': -300, 'Simple': 100}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# You would like to supplement your list of words with other information that you have, which is\n",
    "# many kilobytes, megabytes, or potentially even gigabytes in size. This is technically a right join\n",
    "# if we thought about it in terms of SQL:\n",
    "\n",
    "supplementalData = {\"Spark\":1000, \"Definitive\":200,\"Big\":-300, \"Simple\":100}\n",
    "supplementalData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "822a5ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.broadcast.Broadcast at 0x7fdae968b7c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can broadcast this structure across Spark and reference it by using suppBroadcast. This\n",
    "# value is immutable and is lazily replicated across all nodes in the cluster when we trigger an action:\n",
    "\n",
    "suppBroadcast = spark.sparkContext.broadcast(supplementalData)\n",
    "\n",
    "suppBroadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21ad7ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Spark': 1000, 'Definitive': 200, 'Big': -300, 'Simple': 100}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We reference this variable via the value method, which returns the exact value that we had\n",
    "# earlier. This method is accessible within serialized functions without having to serialize the data.\n",
    "# This can save you a great deal of serialization and deserialization costs because Spark transfers\n",
    "# data more efficiently around the cluster using broadcasts:\n",
    "\n",
    "suppBroadcast.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fad33b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Big', -300),\n",
       " ('The', 0),\n",
       " ('Guide', 0),\n",
       " (':', 0),\n",
       " ('Data', 0),\n",
       " ('Processing', 0),\n",
       " ('Made', 0),\n",
       " ('Simple', 100),\n",
       " ('Definitive', 200),\n",
       " ('Spark', 1000)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we could transform our RDD using this value. In this instance, we will create a key–value\n",
    "# pair according to the value we might have in the map. \n",
    "# If we lack the value, we will simply replace it with 0\n",
    "\n",
    "words.map(lambda word: (word, suppBroadcast.value.get(word, 0))).sortBy(lambda wordPair: wordPair[1]).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387e0cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only difference between this and passing it into the closure is that we have done this in a\n",
    "# much more efficient manner (Naturally, this depends on the amount of data and the number of\n",
    "# executors. For very small data (low KBs) on small clusters, it might not be). Although this small\n",
    "# dictionary probably is not too large of a cost, if you have a much larger value, the cost of\n",
    "# serializing the data for every task can be quite significant.\n",
    "# One thing to note is that we used this in the context of an RDD; we can also use this in a UDF or\n",
    "# in a Dataset and achieve the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56defb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulators\n",
    "# Accumulators (Figure 14-2), Spark’s second type of shared variable, are a way of updating a\n",
    "# value inside of a variety of transformations and propagating that value to the driver node in an\n",
    "# efficient and fault-tolerant way.\n",
    "# Accumulators provide a mutable variable that a Spark cluster can safely update on a per-row\n",
    "# basis. You can use these for debugging purposes (say to track the values of a certain variable per\n",
    "# partition in order to intelligently use it over time) or to create low-level aggregation.\n",
    "# Accumulators are variables that are “added” to only through an associative and commutative\n",
    "# operation and can therefore be efficiently supported in parallel. You can use them to implement\n",
    "# counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types,\n",
    "# and programmers can add support for new types.\n",
    "# For accumulator updates performed inside actions only, Spark guarantees that each task’s update\n",
    "# to the accumulator will be applied only once, meaning that restarted tasks will not update the\n",
    "# value. In transformations, you should be aware that each task’s update can be applied more than\n",
    "# once if tasks or job stages are reexecuted.\n",
    "# Accumulators do not change the lazy evaluation model of Spark. If an accumulator is being\n",
    "# updated within an operation on an RDD, its value is updated only once that RDD is actually\n",
    "# computed (e.g., when you call an action on that RDD or an RDD that depends on it).\n",
    "# Consequently, accumulator updates are not guaranteed to be executed when made within a lazy\n",
    "# transformation like map().\n",
    "# Accumulators can be both named and unnamed. Named accumulators will display their running\n",
    "# results in the Spark UI, whereas unnamed ones will not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44f959cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Basic Example\n",
    "# Let’s experiment by performing a custom aggregation on the Flight dataset that we created\n",
    "# earlier in the book. In this example, we will use the Dataset API as opposed to the RDD API, but\n",
    "# the extension is quite similar:\n",
    "\n",
    "flights = spark.read.parquet(\"/data/flight-data/parquet/2010-summary.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "011d45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now let’s create an accumulator that will count the number of flights to or from China. Even\n",
    "# though we could do this in a fairly straightfoward manner in SQL, many things might not be so\n",
    "# straightfoward. Accumulators provide a programmatic way of allowing for us to do these sorts of\n",
    "# counts. The following demonstrates creating an unnamed accumulator:\n",
    "\n",
    "accChina = spark.sparkContext.accumulator(0)\n",
    "\n",
    "# Our use case fits a named accumulator a bit better. There are two ways to do this: a short-hand\n",
    "# method and a long-hand one. The simplest is to use the SparkContext. Alternatively, we can\n",
    "# instantiate the accumulator and register it with a name:\n",
    "\n",
    "accChina = new LongAccumulator\n",
    "accChina2 = spark.sparkContext.longAccumulator(\"China\")\n",
    "spark.sparkContext.register(accChina, \"China\")\n",
    "\n",
    "# We specify the name of the accumulator in the string value that we pass into the function, or as\n",
    "# the second parameter into the register function. Named accumulators will display in the Spark\n",
    "# UI, whereas unnamed ones will not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddb07e55",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3043183892.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_34444/3043183892.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    The next step is to define the way we add to our accumulator. This is a fairly straightforward\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "The next step is to define the way we add to our accumulator. This is a fairly straightforward\n",
    "function:\n",
    "\n",
    "def accChinaFunc(flight_row):\n",
    "    destination = flight_row[\"DEST_COUNTRY_NAME\"]\n",
    "    origin = flight_row[\"ORIGIN_COUNTRY_NAME\"]\n",
    "    if destination == \"China\":\n",
    "        accChina.add(flight_row[\"count\"])\n",
    "    if origin == \"China\":\n",
    "        accChina.add(flight_row[\"count\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now, let’s iterate over every row in our flights dataset via the foreach method. The reason for\n",
    "# this is because foreach is an action, and Spark can provide guarantees that perform only inside\n",
    "# of actions.\n",
    "\n",
    "# The foreach method will run once for each row in the input DataFrame (assuming that we did\n",
    "# not filter it) and will run our function against each row, incrementing the accumulator\n",
    "# accordingly:\n",
    "\n",
    "# flights.foreach(lambda flight_row: accChinaFunc(flight_row))\n",
    "# This will complete fairly quickly, but if you navigate to the Spark UI, you can see the relevant\n",
    "# value, on a per-Executor level, even before querying it programmatically, as demonstrated in\n",
    "\n",
    "# Of course, we can query it programmatically, as well. To do this, we use the value property:\n",
    "\n",
    "accChina.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ae6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd00230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b453b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
