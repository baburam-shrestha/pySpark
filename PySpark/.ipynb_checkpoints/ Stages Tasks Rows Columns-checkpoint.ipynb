{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd14ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured API Overview  \n",
    "#         Datasets\n",
    "#         DataFrames\n",
    "#         SQL tables and views\n",
    "\n",
    "# A Spark Job\n",
    "# Stages\n",
    "# Tasks\n",
    "# Rows \n",
    "# Columns\n",
    "# Spark Types\n",
    "# Directed Acyclic graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b53eb17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 09:18:00 WARN Utils: Your hostname, HP-G62 resolves to a loopback address: 127.0.1.1; using 192.168.18.113 instead (on interface enp3s0)\n",
      "22/10/17 09:18:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 09:18:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/17 09:18:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/17 09:18:04 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/10/17 09:18:04 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/10/17 09:18:04 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/10/17 09:18:04 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/10/17 09:18:04 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "22/10/17 09:18:04 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "22/10/17 09:18:04 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark Types\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b221d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames and Datasets\n",
    "# Spark has two notions of structured collections: \n",
    "#     DataFrames and\n",
    "#     Datasets. \n",
    "\n",
    "# DataFrames and Datasets are (distributed) table-like collections with well-defined rows and\n",
    "# columns. \n",
    "# Each column must have the same number of rows as all the other columns and each column has type information that\n",
    "# must be consistent for every row in the collection. \n",
    "# In Spark, DataFrames and Datasets represent immutable, lazily evaluated plans that specify what operations to apply to data residing at a\n",
    "# location to generate some output. When we perform an action on a DataFrame, \n",
    "# we instruct Spark to perform the actual transformations and return the result. \n",
    "# These represent plans of how to manipulate rows and columns to compute the user’s desired result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9fd5b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schemas\n",
    "# A schema defines the column names and types of a DataFrame.\n",
    "# we can define schemas manually or read a schema from a data source (often called schema on read). \n",
    "# Schemas consist of types, meaning that you need a way of specifying what lies where."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "292daec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Overview of Structured Spark Types\n",
    "# Spark is effectively a programming language of its own. \n",
    "# Internally, Spark uses an engine called Catalyst that maintains its own type information through the planning\n",
    "# and processing of work. \n",
    "# In doing so, this opens up a wide variety of execution optimizations that make significant differences. \n",
    "# Spark types map directly to the different language APIs that Spark maintains and\n",
    "# there exists a lookup table for each of these in Scala, Java, Python, SQL, and R. \n",
    "# Even if we use Spark’s Structured APIs from Python or R, the majority of our manipulations will operate strictly\n",
    "# on Spark types, not Python types. \n",
    "# For example, the following code does not perform addition in Scala or Python; \n",
    "# it actually performs addition purely in Spark:\n",
    "    \n",
    "# in Python\n",
    "df = spark.range(500).toDF(\"number\")\n",
    "df.select(df[\"number\"] + 10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  dataframe versus dataset\n",
    "# https://phoenixnap.com/kb/rdd-vs-dataframe-vs-dataset#:~:text=DataFrames%20are%20a%20SparkSQL%20data,and%20the%20convenience%20of%20RDDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Spark Job\n",
    "# Spark Jobs (?)\n",
    "# User: babucode\n",
    "# Total Uptime: 118.9 h\n",
    "# Scheduling Mode: FIFO\n",
    "#  Event Timeline\n",
    "#  Enable zooming\n",
    "\n",
    "# Spark Jobs:A job is defined as a series of stages combined.\n",
    "# So, what Spark does is that as soon as action operations like collect(), count(), etc., is triggered, \n",
    "# the driver program, which is responsible for launching the spark application as well as considered the \n",
    "# entry point of any spark application, converts this spark application into a single job which can be seen \n",
    "# in the figure below.\n",
    "# https://www.analyticsvidhya.com/blog/2022/09/all-about-spark-jobs-stages-and-tasks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ef377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stages\n",
    "# https://www.analyticsvidhya.com/blog/2022/09/all-about-spark-jobs-stages-and-tasks/\n",
    "# Now here comes the concept of Stage. Whenever there is a shuffling of data over the network, Spark divides the job into multiple stages. \n",
    "# Therefore, a stage is created when the shuffling of data takes place.\n",
    "\n",
    "# These stages can be either processed parallelly or sequentially depending upon the dependencies of these stages between each other. \n",
    "# If there are two stages, Stage 0 and Stage 1, and if they are not sequentially dependent, they will be executed parallelly.\n",
    "\n",
    "# The sequential processing of RDDs in a single stage is called pipelining.\n",
    "\n",
    "# So, in our code, we have used reduceByKey() function, which shuffles our data in order to group the same keys. \n",
    "# Since shuffling of data is taking place only once, our job will be divided into two stages as shown in the figure below.\n",
    "\n",
    "# There are two types of stages in Spark:\n",
    "\n",
    "# 1.ShuffleMapStage in Spark\n",
    "\n",
    "# 2. ResultStage in Spark\n",
    "\n",
    "# 1. ShuffleMapStage\n",
    "# As the name suggests, it is a type of stage in the spark that produces data for shuffle operation.\n",
    "# The output of this stage acts as an input for the other following stages.\n",
    "# In the above code, Stage 0 will act as the ShuffleMapStage since it produces data for shuffle operation, \n",
    "# which acts as an input for Stage 1.\n",
    "\n",
    "# 2. ResultStage in Spark\n",
    "# The final stage in a Job executes an action operation by running a function (in our example, the action operation is collect) on an RDD.\n",
    "# It computes the result of the active operation.\n",
    "# Stage 1 in our example acts as a ResultStage since it gives us a result of an action operation performed on an RDD.\n",
    "# In our code, after the data shuffling, similar keys got grouped using reduceByKey() \n",
    "# function, so this stage gives us the final result of our code using the collect (action operation) function.\n",
    "\n",
    "# A stage is further a group of tasks executed together. Now we will go through what a task is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26c20695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks\n",
    "# https://www.analyticsvidhya.com/blog/2022/09/all-about-spark-jobs-stages-and-tasks/\n",
    "# The single computation unit performed on a single data partition is called a task. \n",
    "# It is computed on a single core of the worker node.\n",
    "# Whenever Spark is performing any computation operation like transformation etc, Spark is executing a task on a partition of data.\n",
    "# Since in our code, we have two partitions of data here, therefore, we have two tasks here.\n",
    "# Each is computing the same operation on a different partition in parallel on a different core of the worker node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a9c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some important points to note:\n",
    "# https://www.analyticsvidhya.com/blog/2022/09/all-about-spark-jobs-stages-and-tasks/\n",
    "# 1. The cluster manager assigns each worker node resources to execute the tasks.\n",
    "# 2. A core is the CPU’s computation unit; it controls the total number of concurrent tasks an executor \n",
    "#     can execute or run.Suppose if the number of cores is 3, then executors can run 3 tasks at max simultaneously.\n",
    "# 3. Executors are responsible for executing tasks individually. Parallel processing of the tasks by an executor depends upon the number of cores assigned to it, as mentioned in the second point.\n",
    "# 4. Each working node has Cache memory for storage, and as soon as a result is computed, it is sent to the driver’s program.\n",
    "\n",
    "# So this is how a Spark application is converted into Job, which is further divided into Stages and Tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c29973d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rows \n",
    "# A row is nothing more than a record of data. \n",
    "# Each record in a DataFrame must be of type Row, as we can see when we collect the following DataFrames. \n",
    "# We can create these rows manually from SQL, from Resilient Distributed Datasets (RDDs), from data sources, or manually from scratch.\n",
    "# Here, we create one by using a range:\n",
    "spark.range(2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20a12c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'number'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns\n",
    "# Columns represent a simple type like an integer or string, a complex type like an array or map, or a null value. \n",
    "# Spark tracks all of this type information for you and offers a variety of ways, with which you can transform columns.\n",
    "# There are a lot of different ways to construct and refer to columns but the two simplest ways are\n",
    "# by using the col or column functions. To use either of these functions, you pass in a column\n",
    "# name:\n",
    "\n",
    "from pyspark.sql.functions import col, column\n",
    "col(\"number\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91433772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'number'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83b44a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Types\n",
    "# We mentioned earlier that Spark has a large number of internal type representations. \n",
    "# We include a handy reference table on the next several pages so that you can most easily reference what type, in your specific language, lines up with the type in Spark.\n",
    "# Before getting to those tables, let’s talk about how we instantiate, or declare, a column to be of a certain type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7de4091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python types at times have certain requirements\n",
    "# To work with the correct Python types, use the following:\n",
    "from pyspark.sql.types import *\n",
    "b = ByteType()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10e40653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ByteType()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde3290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directed Acyclic graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
