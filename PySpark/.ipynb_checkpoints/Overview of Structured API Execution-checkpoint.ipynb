{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85953fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of Structured API Execution\n",
    "# Overview of the steps:\n",
    "# 1. Write DataFrame/Dataset/SQL Code.\n",
    "# 2. If valid code, Spark converts this to a Logical Plan.\n",
    "# 3. Spark transforms this Logical Plan to a Physical Plan, checking for optimizations along the way.\n",
    "# 4. Spark then executes this Physical Plan (RDD manipulations) on the cluster.\n",
    "\n",
    "# To execute code, we must write code. \n",
    "# This code is then submitted to Spark either through the console or via a submitted job. \n",
    "# This code then passes through the Catalyst Optimizer, which decides how the code should be executed and lays out a plan for doing so before, \n",
    "# finally, the code is run and the result is returned to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f5e5f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 09:36:49 WARN Utils: Your hostname, HP-G62 resolves to a loopback address: 127.0.1.1; using 192.168.18.113 instead (on interface enp3s0)\n",
      "22/10/17 09:36:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/17 09:36:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/17 09:36:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/17 09:36:52 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/10/17 09:36:52 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/10/17 09:36:52 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/10/17 09:36:52 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/10/17 09:36:52 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "22/10/17 09:36:52 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "22/10/17 09:36:52 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "22/10/17 09:36:52 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Overview of Structured API Execution\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9cb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Logical Planning\n",
    "# # The first phase of execution is meant to take user code and convert it into a logical plan.\n",
    "# # This logical plan only represents a set of abstract transformations that do not refer to executors or\n",
    "# # drivers, it’s purely to convert the user’s set of expressions into the most optimized version.\n",
    "# # It does this by converting user code into an unresolved logical plan. \n",
    "# This plan is unresolved because although your code might be valid, the tables or columns \n",
    "# that it refers to might or might\n",
    "# # not exist. Spark uses the catalog, a repository of all table and DataFrame information, to resolve\n",
    "# # columns and tables in the analyzer. The analyzer might reject the unresolved logical plan if the\n",
    "# # required table or column name does not exist in the catalog. If the analyzer can resolve it, the\n",
    "# # result is passed through the Catalyst Optimizer, a collection of rules that attempt to optimize the\n",
    "# # logical plan by pushing down predicates or selections. Packages can extend the Catalyst to\n",
    "# # include their own rules for domain-specific optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc0d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical Planning\n",
    "# After successfully creating an optimized logical plan, Spark then begins the physical planning\n",
    "# process. The physical plan, often called a Spark plan, specifies how the logical plan will execute\n",
    "# on the cluster by generating different physical execution strategies and comparing them through\n",
    "# a cost model, as depicted in Figure 4-3. An example of the cost comparison might be choosing\n",
    "# how to perform a given join by looking at the physical attributes of a given table (how big the\n",
    "# table is or how big its partitions are).\n",
    "# Physical planning results in a series of RDDs and transformations. This result is why you might\n",
    "# have heard Spark referred to as a compiler—it takes queries in DataFrames, Datasets, and SQL\n",
    "# and compiles them into RDD transformations for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution\n",
    "# Upon selecting a physical plan, Spark runs all of this code over RDDs, the lower-level\n",
    "# programming interface of Spark . Spark performs further\n",
    "# optimizations at runtime, generating native Java bytecode that can remove entire tasks or stages\n",
    "# during execution. Finally the result is returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340f214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5ebf29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d2909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2fc7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb83916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298635f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
