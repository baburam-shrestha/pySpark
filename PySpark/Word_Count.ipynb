{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count\n",
    "\n",
    "### Counting the number of occurances of words in a text is a popular first exercise using map-reduce.\n",
    "\n",
    "## The Task\n",
    "**Input:** A text file consisisting of words separated by spaces.  \n",
    "**Output:** A list of words and their counts, sorted from the most to the least common.\n",
    "\n",
    "We will use the book \"Moby Dick\" as our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/10 10:41:32 WARN Utils: Your hostname, HP-G62 resolves to a loopback address: 127.0.1.1; using 192.168.18.113 instead (on interface enp3s0)\n",
      "22/10/10 10:41:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/10 10:41:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/10 10:41:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/10 10:41:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/10/10 10:41:35 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/10/10 10:41:35 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/10/10 10:41:35 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "#start the SparkContext\n",
    "from pyspark import SparkContext\n",
    "sc=SparkContext(master=\"local[4]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a plan for pretty print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_plan(rdd):\n",
    "    for x in rdd.toDebugString().decode().split('\\n'):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `textFile()` to read the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.2 ms, sys: 0 ns, total: 6.2 ms\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_file = sc.textFile(\"Moby-Dick.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for counting the words\n",
    "\n",
    "* split line by spaces.\n",
    "* map `word` to `(word,1)`\n",
    "* count the number of occurances of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 202 ms, sys: 16.9 ms, total: 219 ms\n",
      "Wall time: 3.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words =text_file.flatMap(lambda line: line.split(\" \"))\n",
    "not_empty = words.filter(lambda x: x!='') \n",
    "key_values= not_empty.map(lambda word: (word, 1)) \n",
    "key_values.collect()\n",
    "counts=key_values.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 549),\n",
       " ('Project', 79),\n",
       " ('EBook', 1),\n",
       " ('of', 6587),\n",
       " ('Moby', 79),\n",
       " ('is', 1586),\n",
       " ('use', 35),\n",
       " ('anyone', 5),\n",
       " ('anywhere', 11),\n",
       " ('at', 1227),\n",
       " ('no', 447),\n",
       " ('restrictions', 2),\n",
       " ('whatsoever.', 5),\n",
       " ('may', 223),\n",
       " ('it,', 237),\n",
       " ('give', 68),\n",
       " ('away', 117),\n",
       " ('re-use', 2),\n",
       " ('this', 1169),\n",
       " ('online', 4),\n",
       " ('www.gutenberg.org', 2),\n",
       " ('Author:', 1),\n",
       " ('Last', 1),\n",
       " ('January', 1),\n",
       " ('3,', 2),\n",
       " ('Posting', 1),\n",
       " ('Date:', 2),\n",
       " ('#2701]', 1),\n",
       " ('June,', 3),\n",
       " ('Language:', 1),\n",
       " ('English', 42),\n",
       " ('***', 6),\n",
       " ('OF', 59),\n",
       " ('THIS', 12),\n",
       " ('GUTENBERG', 3),\n",
       " ('MOBY', 3),\n",
       " ('DICK;', 3),\n",
       " ('Lazarus', 6),\n",
       " ('Original', 1),\n",
       " ('combination', 2),\n",
       " ('now-defunct', 1),\n",
       " ('project', 3),\n",
       " ('version', 3),\n",
       " ('are', 586),\n",
       " ('University', 1),\n",
       " ('Adelaide', 1),\n",
       " ('preserving', 3),\n",
       " ('version.', 1),\n",
       " ('resulting', 3),\n",
       " ('was', 1566)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap()\n",
    "Note the line:\n",
    "```python\n",
    "words =     text_file.flatMap(lambda line: line.split(\" \"))\n",
    "```\n",
    "Why are we using `flatMap`, rather than `map`?\n",
    "\n",
    "The reason is that the operation `line.split(\" \")` generates a **list** of strings, so had we used `map` the result would be an RDD of lists of words. Not an RDD of words.\n",
    "\n",
    "The difference between `map` and `flatMap` is that the second expects to get a list as the result from the map and it **concatenates** the lists to form the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The execution plan\n",
    "In the last cell we defined the execution plan, but we have not started to execute it.\n",
    "\n",
    "* Preparing the plan took ~100ms, which is a non-trivial amount of time, \n",
    "* But much less than the time it will take to execute it.\n",
    "* Lets have a look a the execution plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the details\n",
    "To see which step in the plan corresponds to which RDD we print out the execution plan for each of the RDDs.  \n",
    "\n",
    "Note that the execution plan for `words`, `not_empty` and `key_values` are all the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) Data/Moby-Dick.txt MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  Data/Moby-Dick.txt HadoopRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "pretty_print_plan(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[23] at RDD at PythonRDD.scala:53 []\n",
      " |  Data/Moby-Dick.txt MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  Data/Moby-Dick.txt HadoopRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "pretty_print_plan(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[24] at RDD at PythonRDD.scala:53 []\n",
      " |  Data/Moby-Dick.txt MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  Data/Moby-Dick.txt HadoopRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "pretty_print_plan(not_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[18] at collect at <timed exec>:4 []\n",
      " |  Data/Moby-Dick.txt MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  Data/Moby-Dick.txt HadoopRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "pretty_print_plan(key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[25] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[22] at mapPartitions at PythonRDD.scala:145 []\n",
      " |  ShuffledRDD[21] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(2) PairwiseRDD[20] at reduceByKey at <timed exec>:5 []\n",
      "    |  PythonRDD[19] at reduceByKey at <timed exec>:5 []\n",
      "    |  Data/Moby-Dick.txt MapPartitionsRDD[7] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "    |  Data/Moby-Dick.txt HadoopRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "pretty_print_plan(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Execution plan   | RDD |  Comments |\n",
    "| :---------------------------------------------------------------- | :------------: | :--- |\n",
    "|`(2)_PythonRDD[6] at RDD at PythonRDD.scala:48 []`| **counts** | Final RDD|\n",
    "|`_/__MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:436 []`| **---\"---** |\n",
    "|`_/__ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:0 [`| **---\"---** | RDD is partitioned by key |\n",
    "|`_+-(2)_PairwiseRDD[3] at reduceByKey at <timed exec>:4 []`| **---\"---** | Perform mapByKey |\n",
    "|`____/__PythonRDD[2] at reduceByKey at <timed exec>:4 []`| **words, not_empty, key_values** | The result of  partitioning into words|\n",
    "| | |  removing empties, and making into (word,1) pairs|\n",
    "|`____/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at Nat`| **text_file** | The partitioned text |\n",
    "|`____/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMeth`| **---\"---** | The text source |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "Finally we count the number of times each word has occured.\n",
    "Now, finally, the Lazy execution model finally performs some actual work, which takes a significant amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=33781, total words=215133, mean no. occurances per word=6.37\n",
      "CPU times: user 26.2 ms, sys: 3.4 ms, total: 29.6 ms\n",
      "Wall time: 2.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Run #1\n",
    "Count=counts.count()  # Count = the number of different words\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y) # \n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amortization\n",
    "When the same commands are performed repeatedly on the same data, the execution time tends to decrease in later executions.\n",
    "\n",
    "The cells below are identical to the one above, with one exception at `Run #3`\n",
    "\n",
    "Observe that `Run #2` take much less time that `Run #1`. Even though no `cache()` was explicitly requested. The reason is that Spark caches (or materializes) `key_values`, before executing `reduceByKey()` because performng reduceByKey requires a shuffle, and a shuffle requires that the input RDD is materialized. In other words, sometime caching happens even if the programmer did not ask for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=33781, total words=215133, mean no. occurances per word=6.37\n",
      "CPU times: user 25.1 ms, sys: 10.5 ms, total: 35.5 ms\n",
      "Wall time: 787 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Run #2\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit Caching\n",
    "In `Run #3` we explicitly ask for `counts` to be cached. This will reduce the execution time in the following run `Run #4` by a little bit, but not by much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=33781, total words=215133, mean no. occurances per word=6.37\n",
      "CPU times: user 33.7 ms, sys: 2.24 ms, total: 36 ms\n",
      "Wall time: 924 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Run #3, cache\n",
    "Count=counts.cache().count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=33781, total words=215133, mean no. occurances per word=6.37\n",
      "CPU times: user 18.4 ms, sys: 7.6 ms, total: 26 ms\n",
      "Wall time: 602 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run #4\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different words=33781, total words=215133, mean no. occurances per word=6.37\n",
      "CPU times: user 15.1 ms, sys: 9.96 ms, total: 25 ms\n",
      "Wall time: 489 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run #5\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
